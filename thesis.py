# -*- coding: utf-8 -*-
"""Thesis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PEq7qlko-WNuWqFrmF063_hKRxVUOlpQ
"""

#login at hugging face to access the model

import huggingface_hub
huggingface_hub.login()

pip install -q -U bitsandbytes
pip install accelerate
!pip install -q bitsandbytes datasets accelerate loralib
!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git
!pip install python-docx
pip install datasets



from docx import Document
from docx.document import Document as _Document
from docx.oxml.text.paragraph import CT_P
from docx.oxml.table import CT_Tbl
from docx.table import _Cell, Table
from docx.text.paragraph import Paragraph
import os

text =[]

# function for determining the type of content: table or paragraph

def iter_block_items(parent):
    if isinstance(parent, _Document):
        parent_elm = parent.element.body
    elif isinstance(parent, _Cell):
        parent_elm = parent._tc
    elif isinstance(parent, _Row):
        parent_elm = parent._tr
    for child in parent_elm.iterchildren():
        if isinstance(child, CT_P):
            yield Paragraph(child, parent)
        elif isinstance(child, CT_Tbl):
            yield Table(child, parent)

# The main cycle'
for i in os.listdir('/content/data'):
  # recieve the documents from directory
  doc = Document('/content/data'+i)
  for block in iter_block_items(doc):
      if isinstance(block, Paragraph):
          for paragraph in doc.paragraphs:
              for run in paragraph.runs:
                if run.bold:
                  # if element is title (bold), append it to list with special tags
                  text.append('<title>' + run.text + '</title>')
                else:
                  text.append(run.text)
      # append tables
      elif isinstance(block, Table):
          for row in block.rows:
              row_data = []
              for cell in row.cells:
                  for paragraph in cell.paragraphs:
                      row_data.append(paragraph.text)
              text.append("".join(row_data))

# delete NaN
text = [value for value in text if value]

# rewrite list to file .txt
with open(r'/content/data.txt', 'w') as fp:
    for item in text:
        if item != ' ':
          # write each item on a new line
          fp.write("%s\n" % item)
    print('Done')

# import model from hugging face
from transformers import AutoTokenizer
MODEL_NAME = 'mistralai/Mistral-7B-v0.1'
# define the tokenizator
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

# convert the file to a dataset
from datasets import load_dataset
datasets = load_dataset("text", data_files={"train": '/content/data.txt'})

# define the function of tokenization
def tokenize_function(examples):
    return tokenizer(examples["text"])

ds_token = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])

import torch
torch.cuda.is_available()

tokenizer.pad_token = tokenizer.eos_token

import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig

MODEL_NAME = 'mistralai/Mistral-7B-v0.1'

# model's configuration
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map='auto',
    quantization_config=bnb_config,
    #low_cpu_mem_usage=True,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# a function for determining the number of trained parameters
def parameters(model):
  all_parameters = 0
  trainable_parameters = 0
  for _, i in model.named_parameters():
    all_parameters += i.numel()
    if i.requires_grad:
      trainable_parameters += i.numel()
  print('Всего параметров:', all_parameters, 'Тренированные параметры:', trainable_parameters, 'Процент:', trainable_parameters/all_parameters*100)

parameters(model)

print(model)

# "freezing weights"
for param in model.parameters():
  param.requires_grad = False

# memory optimization
model.gradient_checkpointing_enable()

# calculating gradients for input data
model.enable_input_require_grads()

from peft import LoraConfig, get_peft_model,  prepare_model_for_kbit_training

# LoRA's configuration
config = LoraConfig(
    r=32,
    lora_alpha=2,
    inference_mode=False,
    target_modules=["q_proj", 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
)

# training mode
config.inference_mode = False

model = get_peft_model(model, config)
print(parameters(model))

# Knowledge-Based Iterative Training
model = prepare_model_for_kbit_training(model)

tokenizer.pad_token = tokenizer.eos_token

import transformers

# Train model
trainer = transformers.Trainer(
    model=model,
    train_dataset=ds_token['train'],
    args=transformers.TrainingArguments(
        per_device_train_batch_size=32, # butch size
        gradient_accumulation_steps=2, # gradient accumulation to save video memory
        fp16=False,
        learning_rate=1e-4,
        output_dir='output',
        max_steps=200,
        num_train_epochs=8,
        logging_steps = 1,
        weight_decay=0.01,
        lr_scheduler_type="constant"
    ),

    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)
)
model.config.use_cache = False
trainer.train()

"""Тип cosine

"""

# disabling cache
model.config.use_cache = False

model.save_pretrained("trained-model")

# redefining the model
from peft import PeftConfig
config = PeftConfig.from_pretrained('./trained-model')
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict=True,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

text = ' Input text '

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

input_ids = tokenizer.encode(text, return_tensors="pt")
input_ids = input_ids.to('cuda')
output = model.generate(input_ids, max_length=100, pad_token_id = 50256, num_return_sequences=1, do_sample=True,  temperature = 0.7, top_p = 0.7, max_new_tokens = 300)

# Decode and print the generated text
generated_text = tokenizer.decode(output[0], skip_special_tokens=False)
print(generated_text)